---
title: "Statistics and Predictive Analytics, Lab 5"
author: "Swati Singh"
output:
  html_notebook
---

```{r message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(caret)
library(arm)
library(MASS)
library(missForest)
library(dplyr)
```

### Introduction

In the lecture and tutorial videos we have discussed regularization and introduced logistic regression.  This lab will give you practice working with both methods, and will give you a chance to review other important techniques such as missing data imputation, cross-validation, and communicating key results.  

### Cleaning and imputing missing data

We will start out working with the "college.csv" dataset in the folder for Lab 5. This dataset consists in statistics for a large number of US Colleges from the 1995 issue of US News and World Report.  The outcome variable is "Apps"---the number of annual applications received by the college. One of the challenges in using this dataset is that it contains missing observations that are best imputed.  The dataset also has some evident data errors.  As noted in the videos, it makes sense to clean the data first (and remove the names of the colleges since that variable, with a different value for every row, will be of no assistance in predicting the number of applications a college receives), then impute the missings. 

```{r}
c <- read.csv("college.csv")
names(c)[1] <- "college"

#You can find a description of the dataset here:
library(ISLR)
?College
```

**Question 1**: Clean the data. Think of logical checks for variable values, for example, after summarizing the data.  If you encounter values that are not possible, then remove those rows. (Another option, if there were large numbers of mistaken values, would be to turn them into NAs and impute them.)   The dataset contains 777 rows. I was able to find 5 erroneous observations, bringing the number of rows down to 772. See if you can do the same. Please indicate which variables you have cleaned and why.

```{r}
# Your code goes here
c['college']<-NULL
summary(c)
#Some applications have negative value. This is not realistic as students were enrolled.
c<-c%>%
  filter(Apps>0)
summary(c)
#Pct columns: Top10perc, Top25perc,PhD, Terminal,perc.alumni, Grad.Rate
#Ensure these are within 0-100. From summary we can see, PhD and Grad.Rate is not. Remove these rows

c<-c%>%
  filter(PhD<=100 & Grad.Rate<=100 )

dim(c)


```

> Some applications have negative value. This is not realistic as students were enrolled. Percentage columns: Top10perc, Top25perc,PhD, Terminal,perc.alumni, Grad.Rate
Ensure these are within 0-100. From summary we can see, PhD and Grad.Rate is not. Remove these rows


In the tutorial I mentioned that it is a good idea to check for near zero variance predictors, since they typically do not add much to a model, and can produce problems during cross-validation. We will use the `nearZeroVar()` function in caret:

```{r}
nearZeroVar(c,  names= T)
```

The predictors appear to have enough variance, given the function's defaults.  This function would, if needed, return a vector of problematic columns for removal.

Use missForest to impute missing values. This resulting imputed dataset is the one we will use for subsequent modeling.

```{r}
set.seed(31)
# Your code for imputing missing data goes here.
c_complete <- missForest(c)$ximp
```

As discussed previously, however, there are other options.  The caret package has support for imputation, but, as we've seen, its imputation functions only handle numeric data.  We can make categorical predictors  numeric by turning them into what are known as dummy variables.  A dummy variable is a binary variable that represents the presence or absence of a given level of a factor or categorical variable; the original variable is thus split into multiple columns.  Here is an illustration using the `dummyVars()` function in caret on a subset of the college dataset and displaying just the top rows.

```{r}
dummyVars("~.", data = c[, 1:2]) %>%
  predict(newdata = c) %>%
  data.frame %>%
  head
```

Notice that this function takes the Private variable and converts it into a numeric binary variable.  For use in a regression, however, we need to make sure that one level in each dummy is missing (this allows us to avoid  perfect collinearity and the so-called "dummy variable trap").  Here is the amended code using the `fullRank = T` argument:


```{r}
dummyVars("~.", data = c[, 1:2], fullRank = T) %>%
  predict(newdata = c) %>%
  data.frame %>%
  head
```

Of course, this example is not very interesting because we have merely taken a binary categorical variable and turned it into a numeric binary variable.  Still, the example shows what our workflow would be using caret for imputation: 

1. Transform categorical variables into dummy variables, 

2. Impute missing data,

3. Use the new dataset in regression.  

Here is the workflow:

```{r}
# Create dummies 
cdummy <- dummyVars("~.", data = c, fullRank = T) %>% 
  predict(newdata = c) %>%
  data.frame 

# The imputation stage
cdummy_median <- cdummy %>%  
  preProcess(method = "medianImpute") %>%
  predict(newdata = cdummy)

head(cdummy_median)

# Or, the imputation stage (using random forest imputation)
cdummy_bag <- cdummy %>%  
  preProcess(method = "bagImpute") %>%
  predict(newdata = cdummy)

head(cdummy_bag)

# The modeling stage
lm(Apps ~ .,  data = cdummy_bag) %>%
  display

```


Notice that n = 772.  We successfully imputed the missing data, including 5 missing observations in Private.

### Linear regression and regularization

**Question 2**: Fit a regression  model of Apps (the number of applications a college receives) using all the predictor variables in the data set.  Report the cross-validation estimate of out-of-sample RMSE automatically produced by caret. Use 10-fold cross-validation repeated 5 times to ensure good stability in the estimates. Use 31 as the random seed.


```{r}
set.seed(31)
# Your code goes here
(lm_caret <- train(Apps ~ .,
      data=c_complete,
      preProcess=c("center","scale"),
      trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5),
      method="lm"))

```

>The cross-validation estimate of out-of-sample RMSE is `r lm_caret$results$RMSE`

**Question 3**:  Fit a regularized regression  model of Apps  using `glmnet`  in caret.  Use all the predictor variables, but do not specify a search grid or a particular model (lasso or ridge); let the defaults in glmnet handle these choices. Again, use 10 fold cross validation repeated 5 times. 

1. Report estimated out-of-sample RMSE.

2. Report which model type---ridge or lasso or a mixture of the two---was chosen by glmnet.

3. Is there a reason to prefer one of these models--- linear regression (question 2) or this regularized model--- for prediction?

```{r}
set.seed(410)
# Your code goes here
(reg_lm<-train(Apps ~ .,
      data=c_complete,
      preProcess=c("center","scale"),
      trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5),
      metric="RMSE",
      method="glmnet"))

Tuned_Model_RMSE<-min(reg_lm$results$RMSE)
```

>1. Report estimated out-of-sample RMSE.:`r Tuned_Model_RMSE`
2. The model chose elastic net which is a combination of lasso and ridge. This is because the final model usestwo tuning parameters alpha = 0.1 and lambda = 7.319107.
3.We use regularization methods when we want to add a shrinkage penalty that allows the less contributing variable coefficient to shrink towards zero. In this case, we can see that the R2 has slightly changed when we switched from linear model to regularized model by 0.002 which is a negligible change. Since regaularized model is not contributing substantially in improving our model, we should choose linear regression model for prediction. "Everything should be made simple as possible, but not simpler - Albert Einstein"


**Question 4**:  Which predictor had the largest effect size in the linear model (from question 2)?

```{r}
# Your code goes here

lm_caret$finalModel
```
>Accept has the largest effect size of abs value 3907 followed by Top10perc and Enroll respectively.

### Statistical communication 

**Question 5**: Create and explain a visualization that conveys what you think is the main result from the above---or any additional---modeling. 

```{r}
# Your code goes here
par(mfrow=c(2,2))
ggplot(c, aes(x=Accept, y=Apps)) +
    geom_point() +
    geom_smooth(method="lm",   
                se=FALSE) +
  labs(x="No of applications accepted",
       y="No of applications received",
       title="Applications received vs Applications Accepted")

c%>%
  mutate(Acceptance_rate=Accept*100/Apps)%>%
  ggplot(aes(x=Acceptance_rate, y=Apps)) +
    geom_point() +
    geom_smooth(method="lm",  
                se=FALSE) +
  labs(x="Acceptance Rate",
       y="No of applications received",
       title="Acceptance Rate vs Application received")
```

> There is strong positive linear relationship between Apps and Accept. With an increase in number of applications accepted, the number of applications increase. However, contradicting this, as the acceptance rate increases, the number of  applications decreses. there is a weak negative linear relationship between Acceptance rate and applications received.   

```{r}
c%>%
  mutate(ValedictorianIntake=cut_interval(Top10perc,n=3,labels=c("Low","Medium","High")))%>%
  ggplot(aes(x=Apps, y=Accept, color=(ValedictorianIntake))) +
  geom_point() +
  geom_smooth(method="lm",   # Add linear regression lines
                se=FALSE) +
  labs(x="No of applications received",
       y="No of applications accepted",
       title="Applications received vs Applications Accepted")


```
> Schools that have a low intake of valdictorians have a higher number of applications received.Also most schools with high valedictorian intake received a higher number of applications.

```{r}

c%>%
  mutate(Enrollment_rate=Enroll*100/Accept,Enrolled=cut_interval(Enrollment_rate,n=3,labels=c("Low","Medium","High")))%>%
  ggplot(aes(x=Accept, y=Apps,color=Enrolled)) +
    geom_point() +
    geom_smooth(method="lm",  
                se=FALSE) 
 
```
>Schools with lower acceptance received high number of applications and had higher enrollment. Whereas schools with high acceptance and high application count have lower enrollment. It is contrversial to assume that students apply to multiple higher acceptance colleges as backup but may not enroll if they are admitted to the colleges which are difficult to get in.

### Logistic regression

Download the Pima dataset from the folder for Lab 5 in Canvas. Is any cleaning necessary?  It doesn't look like it. Impute missing observations using the median of each variable (do not use missForest). Remember to remove the first column, which is just a row number and, as such, useless for modeling.


```{r}
# Your code  should go here.  
p<-read.csv('pima.csv')
p[1]<-NULL
#Check for NA values
colSums(is.na(p))

#There is no missing data
#summary(p)
#imputeMissings::impute(p,method = "median/mode")
#head(p)
#summary(p)
```

**Question 6**: Using the cleaned and imputed Pima dataset, fit a model with centered and scaled inputs that predicts type using  all the predictors (degrees of freedom = 524). We'll call this the "full model."  Next, create a new model that includes age as a categorical predictor (binned into quartiles). We'll call this the "cat model" (for categorical). The easiest way to do bin age is to use the `quantile()` function as an argument within the `cut()` function. Be sure to use `include.lowest = T` as an argument to cut().  It would look something like this, with `x` as the data object:  `cut(data$x, quantile(data$x), include.lowest = T)`.  Center and scale the predictors in this model. Which of the two models --- the one with age as a continuous (full model) or the one with age as a categorical variable (cat model) ---is better?  Use AIC to answer the question.

```{r}
# Your code goes here
#Since the target variable is categorical, we use generalized linear model
full_model<-glm(type~npreg+glu+bp+skin+bmi+ped+age,
                data=p,family = "binomial")%>%
  standardize
paste("AIC of full model: " ,full_model$aic)

p_age<-p%>%
  mutate(age=cut(age,breaks=quantile(age),include.lowest = T))

cat_model<-glm(type~npreg+glu+bp+skin+bmi+ped+age,data=p_age,family="binomial")%>%standardize
paste("AIC of cat model: " ,cat_model$aic)
```

> Cat_model has a lower AIC than full model.The model with age as categorical predictor is a better fitting model.AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth.

**Question 7**: Interpret the coefficients for the categorical age variable in the above cat model (522 df) as log odds. You should be interpreting 3 coefficients, plus the intercept. By "interpret" I mean that you should explain the coefficients---the actual numbers in the model output: what do they signify? 
```{r}
summary(cat_model)
```

> The log odds of having diabetes is -1.99 when all the other variables are averaged at 0 i.e. have no effect on the target variable. The log odds of having diabetes increases by 0.66 if the candidate changes to age group of 23-28 from the intercept level. The log odds of having diabetes increases by 1.60 if the candidate changes to age group of 28-38 from the intercept level. The log odds of having diabetes increases by 1.58 if the candidate changes to age group of 38-81 from the intercept level.

As I said in the video lecture, converting log odds into probabilities can be tricky.  With log odds we can get a sense of the relative magnitude of effect sizes (some are bigger than others), or whether an effect is positive or negative.  Otherwise, log odds aren't meaningful.  We must translate them into  probabilities.  We do that using the model equation and the inverse logit function.

For example, to calculate the probability that someone who is average in all the predictors has diabetes we could use the intercept from a centered model. This example uses the above model with categorical age (cat_model). I am using the imputed dataset, which I have titled "p_imp."

```{r}

coef(cat_model) # Here are all the coefficients

coef(cat_model)[1] # Here is just the intercept

# But we have a categorical predictor for age, so we need to find
# the category that contains mean age. 

mean(p$age)

# Mean age of 31.6 would be in the third age category or the 9th
# coefficient.

coef(cat_model)[9]

# Here is the log odds of an average person having diabetes:

coef(cat_model)[1] + coef(cat_model)[9]

# Then we use the invlogit function to wrap the equation and 
# transform log odds into a probability :

invlogit <- function(x) exp(x)/(1 + exp(x)) # define the function

invlogit(coef(cat_model)[1] + coef(cat_model)[9]) # wrap the equation

# or, the same thing, turning off the coefficients we do not 
# want (by multiplying by 0) and turning on the ones we do
# want (by multiplying by 1).

invlogit(coef(cat_model)[1] + 
           coef(cat_model)[2]*0 + 
           coef(cat_model)[3]*0 + 
           coef(cat_model)[4]*0 + 
           coef(cat_model)[5]*0 + 
           coef(cat_model)[6]*0 + 
           coef(cat_model)[7]*0 + 
           coef(cat_model)[8]*0 + 
           coef(cat_model)[9]*1 +
           coef(cat_model)[10]*0)

```

Thus, according to this model, someone who is average in all respects has a .4 probability of having diabetes.

**Question 8**: Using the same cat model calculate the probability of diabetes for someone who has a scaled BMI of .3 (assuming that scaling has been done by dividing by 2 sd) but who is average in all the other predictors.  

```{r}
# Your code goes here
invlogit(coef(cat_model)[1] + 
           coef(cat_model)[2]*0 + 
           coef(cat_model)[3]*0 + 
           coef(cat_model)[4]*0 + 
           coef(cat_model)[5]*0 + 
           coef(cat_model)[6]*0.3 + 
           coef(cat_model)[7]*0 + 
           coef(cat_model)[8]*0 + 
           coef(cat_model)[9]*1 +
           coef(cat_model)[10]*0)
```
 
> The probability of diabetes for an average person with a scaled BMI of .3 is 0.49

**Question 9**: Using the same model, calculate the *change* in the probability of diabetes associated with increasing scaled BMI from 0 (average) to .3 for someone who is average in all the other predictors.  

```{r}
# Your code goes here
q9<-(invlogit(coef(cat_model)[1] + 
           coef(cat_model)[2]*0 + 
           coef(cat_model)[3]*0 + 
           coef(cat_model)[4]*0 + 
           coef(cat_model)[5]*0 + 
           coef(cat_model)[6]*0.3 + 
           coef(cat_model)[7]*0 + 
           coef(cat_model)[8]*0 + 
           coef(cat_model)[9]*1 +
           coef(cat_model)[10]*0)  -invlogit(coef(cat_model)[1] + 
           coef(cat_model)[2]*0 + 
           coef(cat_model)[3]*0 + 
           coef(cat_model)[4]*0 + 
           coef(cat_model)[5]*0 + 
           coef(cat_model)[6]*0 + 
           coef(cat_model)[7]*0 + 
           coef(cat_model)[8]*0 + 
           coef(cat_model)[9]*1 +
           coef(cat_model)[10]*0))
q9
```

> The probability increases by `r q9`

